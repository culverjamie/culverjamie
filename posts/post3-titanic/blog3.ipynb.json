{"title":"The Age of Misinformation. A Data Scientist's Role.","markdown":{"yaml":{"title":"The Age of Misinformation. A Data Scientist's Role.","date":"2024-10-25","description":"An investigation into online misinformation and how data science is involved.","categories":["Python"]},"containsRefs":false,"markdown":"\n\nThe Age of Misinformation: A Data Scientist's Role\n\nNumbers have an inherit power to them. They bring a sense power and credibility to the arguement, whether it is genuine or not. Be in an advertisement, news article, or post on social media, the inclusion of statistics makes us more inclined to believe the information at face value. This perception stems from the beleif that \"the facts\" are objective and immune to manipulation.\n\nThe reality is that numbers are so easily manipulated. They can be selective to show exactly what a company wants, misrepresented in visuals to seem more or less extreme, or be flat out changed to shape the result. Context, methodology, and interpretation are purposefully hidden or ignored for the sake of convincing consumers. \n\nIn this post I'll be exploring and demonstrating how simple it is to manipulate data into fitting my narrative. Hopefully this will make people approach data and statistics in media with a critical eye.\n\nFor this example I'll be working on a titanic dataset. This is a labeled dataset meaning we know the correct answers for our predictor variable \"Surived\". In addition, because the dataset is historical, we know the data to be unbiased. This specific dataset does not include all the people on the titanic, but it is a random sample of the full data. To start I'm importing my libraries, doing simple exploration, and cleaning where needed.\n\nLuckily there are no missing values we have to fix. In this next step all I'm doing is changing the columns to be useable in our models later on. 0 will represent Male passengers and 0 1 will represent female passengers.\n\nFor the last step in cleaning, we'll be checking the \"Fare\" column for any outliers because that could have a huge affect on our model.\n\nUnfortunately, there are many outliers on the high end for fares. For this dataset, it would not be appropriate to remove or replace any of our outliers because we could be losnig valueable information about what influences the predictor variable \"Survived\". However, we still need to perform outlier detection to understand the impact it will have on our model later. Knowing we have so many high end outliers in our data could provide insight when we evaluate later on\n\nNow that our dataset is ready, we're going to make a simple train/test split to use in our models. Our predictor variable will be \"Survived\", where 0 represents a passenger did not live and 1 represents a survivor. I'm using a decision tree that is limited to 3 splits for the sake of a simple visualization.\n\nNow that our model has ran we're going to look at it's performance. Please note that LogisticRegression is being used here for the sake of simplicity for our example. LogisticRegression assumes the variables are independent of each other, meaning one value has no affect on the others. For our dataset, the x variables are independent so we don't have to worry about that. It is also sensitive to outliers. We screened our float value, \"Fare\" for outliers earlier and found many. However, as discussed earlier, it is not appropriate to remove these so we will be evaluating with that in mind.\n\nWe'll be looking at the overall accuracy score and a confusion matrix to evaluate. The accuracy score is straightforward, its the number of correct predictions over the total points predicted. \n\nAlthough far from perfect, a (rounded) accuracy score of 80% is a fairly good performance for our LogisticRegression model. In other words, our model correctly predicted 143 points of our 179 included in ytest. I'll discuss this more later, but note that even our labeled, unedited data fails to predict points perfectly.\n\nNext we'll be looking at a confusion matrix. This provides a simple visual to see how our model is predicting points. This will provide insight into what types of points the model is best at predicting and where it struggles.\n\nissues:\n- despite using 'pure' data, where we know the answers, the predictor model is not 100% accurate because errors are going to happen no matter what\n- this alone is enough to be skeptical of peoples numbers\n- but it can get so much worse too\n\nintroduce falsifying data\n- now say the titanic happened today, and we work for a news outlet that wants to push the idea that the famous line 'women and children' was true, regardless of class/ticket\n- the real data did not look good for this claim, or we want it to look even better\n- so i'll edit the data to show all women survived the crash\n\nnow we'll run the same decision tree, same parameters, on our edited dataset\n\n\n- changes the results completely\n- pushes our narrative\n- took 2 seconds to change the entire story\n\n- for this demonstration we know its not true, and its historical data so changing results doesn't really have an impact\n- but say we instead worked for an insurance company, and we were told to decrease the amount paid out on claims. or we worked in sales and our boss asked us to make the sales figures look better.\n- then it actually impacts people. claims who really need it are rejected and the boss looks good on paper while the business is actually failing\n- it becomes the ethical responsibility of data scientists to uphold truthful data and results\n\n- need to ensure the data source is good\n    - reference the LA police deployment issue\n\n- it is unethical to change the data for your own gain\n- causes issues in real life, rapid spread of misinformation\n- not only an issue with numbers, but journalism have had this issue forever. numbers just make it more believeable\n\n- also the NEW responsibility of people today, especially online, to be wary of the numbers/data because its not always true\n- need to watch for the source of the data, the scale in how its displayed, what its being compared to, etc\n- all too easy to make stuff up, to safely navigate the internet people need to understand this\n- data scientist job to not make it any harder on people\n","srcMarkdownNoYaml":"\n\nThe Age of Misinformation: A Data Scientist's Role\n\nNumbers have an inherit power to them. They bring a sense power and credibility to the arguement, whether it is genuine or not. Be in an advertisement, news article, or post on social media, the inclusion of statistics makes us more inclined to believe the information at face value. This perception stems from the beleif that \"the facts\" are objective and immune to manipulation.\n\nThe reality is that numbers are so easily manipulated. They can be selective to show exactly what a company wants, misrepresented in visuals to seem more or less extreme, or be flat out changed to shape the result. Context, methodology, and interpretation are purposefully hidden or ignored for the sake of convincing consumers. \n\nIn this post I'll be exploring and demonstrating how simple it is to manipulate data into fitting my narrative. Hopefully this will make people approach data and statistics in media with a critical eye.\n\nFor this example I'll be working on a titanic dataset. This is a labeled dataset meaning we know the correct answers for our predictor variable \"Surived\". In addition, because the dataset is historical, we know the data to be unbiased. This specific dataset does not include all the people on the titanic, but it is a random sample of the full data. To start I'm importing my libraries, doing simple exploration, and cleaning where needed.\n\nLuckily there are no missing values we have to fix. In this next step all I'm doing is changing the columns to be useable in our models later on. 0 will represent Male passengers and 0 1 will represent female passengers.\n\nFor the last step in cleaning, we'll be checking the \"Fare\" column for any outliers because that could have a huge affect on our model.\n\nUnfortunately, there are many outliers on the high end for fares. For this dataset, it would not be appropriate to remove or replace any of our outliers because we could be losnig valueable information about what influences the predictor variable \"Survived\". However, we still need to perform outlier detection to understand the impact it will have on our model later. Knowing we have so many high end outliers in our data could provide insight when we evaluate later on\n\nNow that our dataset is ready, we're going to make a simple train/test split to use in our models. Our predictor variable will be \"Survived\", where 0 represents a passenger did not live and 1 represents a survivor. I'm using a decision tree that is limited to 3 splits for the sake of a simple visualization.\n\nNow that our model has ran we're going to look at it's performance. Please note that LogisticRegression is being used here for the sake of simplicity for our example. LogisticRegression assumes the variables are independent of each other, meaning one value has no affect on the others. For our dataset, the x variables are independent so we don't have to worry about that. It is also sensitive to outliers. We screened our float value, \"Fare\" for outliers earlier and found many. However, as discussed earlier, it is not appropriate to remove these so we will be evaluating with that in mind.\n\nWe'll be looking at the overall accuracy score and a confusion matrix to evaluate. The accuracy score is straightforward, its the number of correct predictions over the total points predicted. \n\nAlthough far from perfect, a (rounded) accuracy score of 80% is a fairly good performance for our LogisticRegression model. In other words, our model correctly predicted 143 points of our 179 included in ytest. I'll discuss this more later, but note that even our labeled, unedited data fails to predict points perfectly.\n\nNext we'll be looking at a confusion matrix. This provides a simple visual to see how our model is predicting points. This will provide insight into what types of points the model is best at predicting and where it struggles.\n\nissues:\n- despite using 'pure' data, where we know the answers, the predictor model is not 100% accurate because errors are going to happen no matter what\n- this alone is enough to be skeptical of peoples numbers\n- but it can get so much worse too\n\nintroduce falsifying data\n- now say the titanic happened today, and we work for a news outlet that wants to push the idea that the famous line 'women and children' was true, regardless of class/ticket\n- the real data did not look good for this claim, or we want it to look even better\n- so i'll edit the data to show all women survived the crash\n\nnow we'll run the same decision tree, same parameters, on our edited dataset\n\n\n- changes the results completely\n- pushes our narrative\n- took 2 seconds to change the entire story\n\n- for this demonstration we know its not true, and its historical data so changing results doesn't really have an impact\n- but say we instead worked for an insurance company, and we were told to decrease the amount paid out on claims. or we worked in sales and our boss asked us to make the sales figures look better.\n- then it actually impacts people. claims who really need it are rejected and the boss looks good on paper while the business is actually failing\n- it becomes the ethical responsibility of data scientists to uphold truthful data and results\n\n- need to ensure the data source is good\n    - reference the LA police deployment issue\n\n- it is unethical to change the data for your own gain\n- causes issues in real life, rapid spread of misinformation\n- not only an issue with numbers, but journalism have had this issue forever. numbers just make it more believeable\n\n- also the NEW responsibility of people today, especially online, to be wary of the numbers/data because its not always true\n- need to watch for the source of the data, the scale in how its displayed, what its being compared to, etc\n- all too easy to make stuff up, to safely navigate the internet people need to understand this\n- data scientist job to not make it any harder on people\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"blog3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.30","theme":"cosmo","title-block-banner":true,"title":"The Age of Misinformation. A Data Scientist's Role.","date":"2024-10-25","description":"An investigation into online misinformation and how data science is involved.","categories":["Python"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}