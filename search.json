[
  {
    "objectID": "posts/post3-titanic/blog3.html",
    "href": "posts/post3-titanic/blog3.html",
    "title": "The Age of Misinformation. A Data Scientist’s Role.",
    "section": "",
    "text": "The Age of Misinformation: A Data Scientist’s Role\nNumbers have an inherent power to them. They bring a sense of power and credibility to the argument, whether it is genuine or not. Be it in an advertisement, news article, or post on social media, the inclusion of statistics makes us more inclined to believe the information at face value. This perception stems from the belief that “the facts” are objective and immune to manipulation.\nThe reality is that numbers are so easily manipulated. They can be selective to show exactly what a company wants, misrepresented in visuals to seem more or less extreme or be flat-out changed to shape the result. Context, methodology, and interpretation are purposefully hidden or ignored for the sake of convincing consumers.\nIn this post, I’ll be exploring and demonstrating how simple it is to manipulate data to fit my narrative. Hopefully, this will make people approach data and statistics in media with a critical eye.\nFor this example, I’ll be working on a titanic dataset. This is a labeled dataset meaning we know the correct answers for our predictor variable “Surived”. In addition, because the dataset is historical, we know the data to be unbiased. This specific dataset does not include all the people on the Titanic, but it is a random sample of the full data. To start, I’m importing my libraries, doing simple exploration, and cleaning where needed.\n\n# import our libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# check out our dataset\ndf = pd.read_csv(\"titanic.csv\")\nprint(df.shape)\ndf.head(5)\n\n(891, 5)\n\n\n\n\n\n\n\n\n\nSurvived\nPclass\nGender\nAge\nFare\n\n\n\n\n0\n0\n3\nmale\n22.0\n7.2500\n\n\n1\n1\n1\nfemale\n38.0\n71.2833\n\n\n2\n1\n3\nfemale\n26.0\n7.9250\n\n\n3\n1\n1\nfemale\n35.0\n53.1000\n\n\n4\n0\n3\nmale\n35.0\n8.0500\n\n\n\n\n\n\n\n\nprint(f\"Missing values per column:\\n{df.isna().sum()}\")\nprint(f\"Column dtypes:\\n{df.dtypes}\")\n\nMissing values per column:\nSurvived    0\nPclass      0\nGender      0\nAge         0\nFare        0\ndtype: int64\nColumn dtypes:\nSurvived      int64\nPclass        int64\nGender       object\nAge         float64\nFare        float64\ndtype: object\n\n\nLuckily, there are no missing values we have to fix. In this next step, all I’m doing is changing the columns to be useable in our models later on. 0 will represent Male passengers and 0 1 will represent female passengers.\n\n# including this next line to avoid errors from future version\npd.set_option('future.no_silent_downcasting', True)\n\n# changing column \"Gender\" to boolean values. \ndf[\"Gender\"] = df[\"Gender\"].replace({\"male\": 0, \"female\": 1})\nprint(df[\"Gender\"].value_counts())\n\nGender\n0    577\n1    314\nName: count, dtype: int64\n\n\nFor the last step in cleaning, we’ll be checking the “Fare” column for any outliers because that could have a huge effect on our model.\n\n# we'll be using a box and whisker plot for outlier detection\nplt.figure(figsize=(8, 6))\nplt.boxplot(df[\"Fare\"].dropna(), patch_artist=True, boxprops=dict(facecolor=\"blue\"))\nplt.title(\"Fare Outliers\")\nplt.xlabel(\"Fare\")\nplt.grid(axis='x')\n\nplt.show()\n\n\n\n\n\n\n\n\nUnfortunately, there are many outliers on the high end for fares. For this dataset, it would not be appropriate to remove or replace any of our outliers because we could be losing valuable information about what influences the predictor variable “Survived”. However, we still need to perform outlier detection to understand the impact it will have on our model later. Knowing we have so many high-end outliers in our data could provide insight when we evaluate later on.\n\nModel 1 - Pure Data\nNow that our dataset is ready, we’re going to make a simple train/test split to use in our models. Our predictor variable will be “Survived”, where 0 represents a passenger did not live and 1 represents a survivor. I’m using a decision tree that is limited to 3 splits for the sake of a simple visualization.\n\nx = df.drop(columns=[\"Survived\"]) # drop y variable, keep the rest\ny = df[\"Survived\"]\n\n# train/test split. 80 train, 20 test.\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size= 0.2, random_state=1)\nprint(f\"xtrain shape: {xtrain.shape}\")\nprint(f\"ytrain shape: {ytrain.shape}\")\nprint(f\"xtest shape: {xtest.shape}\")\nprint(f\"ytest shape: {ytest.shape}\")\n\n# creating and fitting our model\nmdl1 = LogisticRegression(random_state=1)\nmdl1.fit(xtrain, ytrain)\n\n# create predictions\nypred = mdl1.predict(xtest)\n\nxtrain shape: (712, 4)\nytrain shape: (712,)\nxtest shape: (179, 4)\nytest shape: (179,)\n\n\nNow that our model has ran we’re going to look at it’s performance. Please note that LogisticRegression is being used here for the sake of simplicity for our example. LogisticRegression assumes the variables are independent of each other, meaning one value has no affect on the others. For our dataset, the x variables are independent so we don’t have to worry about that. It is also sensitive to outliers. We screened our float value, “Fare” for outliers earlier and found many. However, as discussed earlier, it is not appropriate to remove these so we will be evaluating with that in mind.\nWe’ll be looking at the overall accuracy score and a confusion matrix to evaluate. The accuracy score is straightforward, its the number of correct predictions over the total points predicted.\n\naccuracy = accuracy_score(ytest, ypred)\nprint(f\"Accuracy of Model 1: {accuracy:.5f}\")\n\nAccuracy of Model 1: 0.79888\n\n\nAlthough far from perfect, a (rounded) accuracy score of 80% is a fairly good performance for our LogisticRegression model. In other words, our model correctly predicted 143 points of our 179 included in ytest. I’ll discuss this more later, but note that even our labeled, unedited data fails to predict points perfectly.\nNext we’ll be looking at a confusion matrix. This provides a simple visual to see how our model is predicting points. This will provide insight into what types of points the model is best at predicting and where it struggles.\n\nconf_matrix = confusion_matrix(ytest, ypred)\nprint(conf_matrix)\n\n[[88 18]\n [18 55]]\n\n\nIn the upper left corner of our confusion matrix is the true positives. That means our model correctly predicted a passenger survived in 88 cases. In the lower right corner are our false negatives. Model 1 correctly predicted instances where the passenger died 55 times. These two sections combined are what make up our accuracy score from earlier. Accuracy is the most widely understood metric because it looks at the correct predictions over the incorrect ones. However, this may not be the best metric in all cases because it can make the results look really good, even when the incorrect guesses are very costly. In other words, for models where predicting wrong could have consequences, accuracy scores may not be the best representative of overall performance. In this context, accuracy works well because there are no ramifications if the model guesses correctly.\nDespite having no ramifications from false positives and negatives, its still crucial to understand what those numbers mean in this context. Knowing we have labeled data, and that this is historical so we know it to be true, the model still makes mistakes. While a 79% chance of correct guesses is performing well, in other contexts, consumers need to be aware of the remaining 21% and the impact it could have.\n\n\nModel 2 - Falsified Data\nIf incorrect classification on labeled data doesn’t fully prove my point, I’m going to show just how easy it is to manipulate our data. Say the Titanic happened today, and we work for a news outlet that wants to push the idea that the famous line ‘women and children first’ was completely true, regardless of class/ticket, and the lifeboats were foolproof.\nFrom our original dataset, this is what the number of women who did not survive looks like.\n\ngender_count = df[(df[\"Gender\"] == 1) & (df[\"Survived\"] == 0)].shape[0]\nprint(f\"Number of instances where female passengers did not survive: {gender_count}\")\n\nNumber of instances where female passengers did not survive: 81\n\n\n\nby_gender = df[\"Gender\"].value_counts()\nprint(f\"Total passengers by gender: {by_gender}\")\n\nTotal passengers by gender: Gender\n0    577\n1    314\nName: count, dtype: int64\n\n\n81 of the 314 women present in this dataset did not survive. This doesn’t push our narrative as strongly as we’d like. With just a few lines of code, I can completely change our dataset and the story of the Titanic.\n\nedited_df = df.copy()\n\n# change all rows where \"Gender\" = 1/Female to Survived = 1/True\nedited_df.loc[edited_df[\"Gender\"] == 1, \"Survived\"] = 1 \n\n# verify changes\ngender_count_edited = edited_df[(edited_df[\"Gender\"] == 1) & (edited_df[\"Survived\"] == 0)].shape[0]\nprint(f\"Number of instances where female passengers did not survive: {gender_count_edited}\")\n\nNumber of instances where female passengers did not survive: 0\n\n\nNow we’ll run the same model as we did earlier on the edited data frame. We’ll also look at the same metrics.\n\n# we need to re-do our train test split to use the edited_df\nx_edited = edited_df.drop(columns=[\"Survived\"])\ny_edited = edited_df[\"Survived\"]\n\n# 80 train, 20 test.\nxtrain_edit, xtest_edit, ytrain_edit, ytest_edit = train_test_split(x_edited, y_edited, test_size= 0.2, random_state=1) \n# must use the same random state\n\n# creating and fitting our model\nmdl2 = LogisticRegression(random_state=1)\nmdl2.fit(xtrain_edit, ytrain_edit)\n\n# create predictions\nypred2 = mdl2.predict(xtest_edit)\n\naccuracy2 = accuracy_score(ytest_edit, ypred2)\nprint(f\"Accuracy of Model 2: {accuracy2:.5f}\")\n\nconf_matrix2 = confusion_matrix(ytest_edit, ypred2)\nprint(conf_matrix2)\n\nAccuracy of Model 2: 0.86592\n[[90  0]\n [24 65]]\n\n\nPlease note that we used the same random state for both of our train tests splits, so the rows were sorted into the same subdatasets for this model as the first one. From the accuracy score alone, it’s clear that model 2 performed much better than model 1. In the bottom left corner are our only incorrect classifications. This quadrant represents false positives, where the model predicted they survived when they actually died. This fits our narrative that all women made it off alive much better than the unedited results.\nEditing our dataset completely changed the results of the model and it only took one line of code to make the changes. It is incredibly easy to manipulate data to push our narrative.\nFor this demonstration, we know model 2’s results are not true. It’s also historical data so changing results doesn’t really have an impact. It won’t change the reality of who survived or not. But say we instead worked for an insurance company, and we were told to decrease the amount paid out on claims. Or we worked in sales and our boss asked us to make the figures that affect his bonus look better. Then it actually impacts people. Claims that truly needed it are rejected and the boss looks good on paper while the business is actually failing. Here it becomes the ethical responsibility of data scientists to uphold truthful data and results.\n\n\nA Data Scientist’s Role\nToday news and information are constantly being uploaded, updated, changed, and can be posted by anybody. Although untruthfulness or straight-up lies have always been present in journalism, the sheer volume of news available thanks to the internet has exaggerated this problem. It has quickly become the responsibility of the consumer to approach the news with a skeptical mind. It’s extremely difficult to tell what’s real and what’s not at first glance or without additional research.\nWhat many consumers are unfamiliar with is how seeing a single statistic in an article or social post can influence how they approach what they read. As mentioned earlier, the inclusion of a single digit can make the news source seem more reputable. But as we’ve seen and done now, it is all too easy to falsify this.\nThis is where a data scientist’s responsibilities come in. Its already hard enough for people to navigate the insane amount of information available at all times, it shouldn’t be made any harder for personal or commercial gain. The rapid spread of misinformation can cause fear, outrage, confusion and fuels hatred in todays social climate.\nIt’s part of our jobs to do our due diligence on the sourcing of our data, be aware of any bias present in sampling, and use the best data possible for training our models. Maintaining the integrity of the data and telling the complete truth is a moral obligation. We’re also responsible for how people interpret results. How we choose to display our findings when it comes down to scale, organization, and what we model in the first place all have an effect on how people interpret it."
  },
  {
    "objectID": "posts/post1-openai/blog1-openai.html",
    "href": "posts/post1-openai/blog1-openai.html",
    "title": "Using OpenAI’s API",
    "section": "",
    "text": "Using OpenAI’s API\nOpenAI (known for ChatGPT) has a public API available for queries at a low cost of entry. It is perfect for personal use to keep yourself organized, as a study aide, to draft essays or code, generate creative prompts, assist with summarizing searches, and many more tasks. In this post, I’ll introduce a simple query demonstrating how to access the API.\n\n1. Create OpenAI Account\nHead to the OpenAI website and click to create an account if you don’t already have one. If you have an account from using ChatGPT, log in with the same information. Navigate to the OpenAI Developer Platform page through the products dropdown on the homepage.\n\n\n\nimage.png\n\n\n\n\n2. API Key Section\n\nBilling\nOnce logged in and on the developer page, navigate to the upper right corner and select your profile. Under the “Organization” subheading, select “Billing.” To use this API, each query uses a small amount of credits. The minimum purchase is $5 worth of credits which users report is an average of 750 queries.\n\n\n\nimage.png\n\n\n\n\n\n3. Generate Key\nOnce you have credits to use, navigate to the “API Keys” section under the “Organization” sub-heading. Click the green “Create new secret key” button.\n\n\n\n1.1.png\n\n\nHere you can assign a name, add to a project directory, and set any necessary restrictions. Click the green button at the bottom to continue.\nOn the next window, you’ll receive the key. Make sure to copy and store this in a safe place because you will be unable to access it through the OpenAI website once you close this window. I’ve stored mine in Google Drive.\n\n\n\n1.2.png\n\n\n\nUsing the API\nAfter generating our unique key and purchasing credits, we’re ready to use the API. We’ll start by importing our libraries. OpenAI can be pip installed or included in a conda environment. Set your API key to the same variable name as below otherwise, you won’t be able to use it. You can also use the Python package ‘os’ to interact with your operating system and increase security.\n\n# import libraries\nimport openai\n\n# set api key variable\nopenai.api_key = (\"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\")\n\nQuerying ChatGPT is relatively simple. Use the method “openai.ChatCompletion.create()” to connect with ChatGPT’s model. In this example, I’m using GPT 3.5 Turbo. This version is the most cost-effective for simple queries.\nIn the method, we’ll reference two different “roles”. These represent the user input and the chatbot. User input is “user” and the chatbot is “system”.\nThe next piece of the call is “content”. For the user role, this is simply the question or prompt we are asking. For the system role, this is where you’ll provide the chatbot with the persona, tone, and knowledge constraints, enforce content boundaries, and overall guide how the answers will be. In this example, I’ll be asking a coding question and looking for a short, simple response.\nThe final part of using the API is printing the result. The first part in quotes is optional to give context to the return. - Response is the answer generated by ChatGPT. It is stored in a dictionary key pair. - [“choices”] is part of the response dictionary that maps to the answers provided by the model. - [0] is the index of the query we are extracting. - [“message”] contains the role and content response from ChatGPT. - [“content”] extracts the actual text response.\n\n# establish connection and send query\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",  # specify the model\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a beginner-level Python instructor. Keep answers below 200 words.\"},  # system role\n        {\"role\": \"user\", \"content\": \"Explain how to format a dictionary in Python.\"}  # user input\n        ])\n\n# return response\nprint(\"ChatGPT says:\", response[\"choices\"][0][\"message\"][\"content\"])\n\nChatGPT says: In Python, dictionaries are formatted using curly braces {} and key-value pairs separated by a colon. Here's an example:\n\n```python\n# Creating a dictionary\nmy_dict = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\n# Accessing values in the dictionary\nprint(my_dict[\"name\"])  # Output: Alice\nprint(my_dict[\"age\"])   # Output: 30\nprint(my_dict[\"city\"])  # Output: New York\n\n# Adding a new key-value pair\nmy_dict[\"occupation\"] = \"Engineer\"\n\n# Updating a value\nmy_dict[\"age\"] = 31\n\n# Removing a key-value pair\ndel my_dict[\"city\"]\n\n# Iterating through key-value pairs\nfor key, value in my_dict.items():\n    print(key, \":\", value)\n```\n\nRemember, dictionaries are mutable, unordered collections in Python that store data in key-value pairs.\n\n\nChatGPT model successfully returned an answer to our query fitting the tone of an instructor and staying within the word limit. Now you understand how to access the API, and the different parts of the call, and are on your way to integrating ChatGPT into your Python code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jamie Culver! I’m a Data Analyst based in the Des Moines area. I’ve always enjoyed playing the “data detective” role and I love tackling difficult problems. I’m a recent graduate from the Des Moines Area Community College with a certificate in Data Science. Check out my portfolio and personal projects [here.] (https://culverjamie.github.io/culverjamie/)\nI’m currently working in the service industry and I’m open to work! On-site in central Iowa and remote work."
  },
  {
    "objectID": "about.html#contact-me",
    "href": "about.html#contact-me",
    "title": "About",
    "section": "Contact Me:",
    "text": "Contact Me:\nEmail: jamieculver03@gmail.com [LinkedIn:] (linkedin.com/in/jamie-culver-a924a12aa) Cell: 515-556-4451"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tech Jungle: My Exploration Through Data Science",
    "section": "",
    "text": "Exploring Prophet Library\n\n\n\n\n\n\nPython\n\n\nPrediction\n\n\nLibraries\n\n\n\nAn exploration of how Prophet time series forecasting works in python.\n\n\n\n\n\nDec 5, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nUsing OpenAI’s API\n\n\n\n\n\n\nPython\n\n\nLLMs\n\n\nMachine Learning\n\n\n\nA beginners guide to using OpenAI’s API for simple query.\n\n\n\n\n\nNov 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nThe Age of Misinformation. A Data Scientist’s Role.\n\n\n\n\n\n\nPython\n\n\nEthics\n\n\nThink Piece\n\n\n\nAn investigation into online misinformation and how data science is involved.\n\n\n\n\n\nOct 25, 2024\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post2-prophet/blog2-prophet.html",
    "href": "posts/post2-prophet/blog2-prophet.html",
    "title": "Exploring Prophet Library",
    "section": "",
    "text": "Exploring Prophet Library\nProphet is an open-source library for Python and R developed by Facebook. It uses sklearn to operate, and it is designed to make time series analysis simple, easy to interpret, and maintain robust outputs. It is also less computationally expensive and runs much faster than other algorithms. This makes the library appealing for quick and easy but accurate predictions in all sorts of applications. I’ll be diving into the basics of how the library works so it can be used in practice.\n\nHow It Works\nProphet is built off of a fairly simple equation. Each part is explained below.\n\ny(t) = c(t) + s(t) + h(t) + x(t) + E\n\ny: predictor variable\nt: time\nc: trend, overall direction, piecewise (linear with changepoints) or logistic growth.\ns: seasonal effects, recurring patterns found at regular intervals.\nh: holiday effects, domain-specific recurring patterns surrounding holidays.\nx: external regressors, accounts for factors outside the time series that may affect y.\nE: error, residuals resulting from noise or patterns not captured in the other parts.\n\n\n\n\nGetting Started\nProphet was built with ease of use in mind, its not difficult to get started. There are a few rules to follow but past that its straightforward. - Prophet requires a date column formatted as YYYY-MM-DD. - This column must be called ‘ds’ for the Prophet object to recognize it. - The predictor variable column must be called ‘y’. - Other columns/regressors can be added to the data frame using the add_regressor() function. We won’t do this in our example to keep it straightforward.\n\n# import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom prophet import Prophet\nimport seaborn as sns\n\n# import data\ndf = pd.read_csv(\"car_sales.csv\")\n\n# rename for compatibility\ndf.rename(columns={\"Date\": \"ds\"}, inplace=True) \ndf.rename(columns={\"Price ($)\": \"y\"}, inplace=True)\n\n# drop columns we won't use in this demo\ndf = df[[\"ds\", \"y\"]]\n\n# verify\ndf.head()\n\nc:\\Users\\jamie\\miniconda3\\envs\\blog2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nImporting plotly failed. Interactive plots will not work.\n\n\n\n\n\n\n\n\n\nds\ny\n\n\n\n\n0\n1/2/2022\n26000\n\n\n1\n1/2/2022\n19000\n\n\n2\n1/2/2022\n31500\n\n\n3\n1/2/2022\n14000\n\n\n4\n1/2/2022\n24500\n\n\n\n\n\n\n\n\n# convert to datetime\ndf['ds'] = pd.to_datetime(df['ds'], format='%m/%d/%Y')\n\n# verify\nprint(df['ds'].head())\nprint(df['ds'].dtype)\n\n0   2022-01-02\n1   2022-01-02\n2   2022-01-02\n3   2022-01-02\n4   2022-01-02\nName: ds, dtype: datetime64[ns]\ndatetime64[ns]\n\n\nProphet is sensitive to outliers, so we’re going to check for those in ‘y’ before continuing. The dataset is large enough that we’ll remove any outliers.\n\n# pre filter max\ndf['y'].max()\n\n85800\n\n\n\n# check for outliers\nsns.boxplot(y=df['y'])\nplt.title(\"Pre Filter Outliers\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# set IQR boundaries\nq1 = df[\"y\"].quantile(0.25) \nq3 = df[\"y\"].quantile(0.75)\nIQR = q3 - q1\n\n# set upper and lower bound\nlower_bound = q1 - 1.5 * IQR\nupper_bound = q3 + 1.5 * IQR\n\n# remove the outliers\ndf = df[(df['y'] &gt;= lower_bound) & (df['y'] &lt;= upper_bound)]\n\n# reset index to account for removed rows\ndf.reset_index(drop=True, inplace=True)\n\n# verify changes\nsns.boxplot(y=df['y'])\nplt.title(\"Post Filter Outliers\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# post filter max\ndf['y'].max()\n\n57990\n\n\nThe default setting of IQR, upper bound, and lower bounds didn’t catch all of the outliers but it significantly reduced them. While a car price of $57,990 is still decently high, its much more reasonable than the pre-filter max of $85,800. We’ll leave post-filter outliers alone.\nNow we’ll create our train and test split for the modeling. For time series analysis, this looks a little different than what we’re used to. We’ll need to split our data by a certain date to accurately represent the type of predictions it would be used for in practice.\n\nprint(df['ds'].min()) # earliest date in data\nprint(df['ds'].max()) # latest date in data\n\n2022-01-02 00:00:00\n2023-12-31 00:00:00\n\n\nThe dataset spans about 2 years. At 730 days total, to make our train subset 80% of the data, our split date will be August 7th, 2023. Anything beyond that point will be our test set.\n\ndf.index = pd.to_datetime(df.index)\n\nsplit_date = \"2023-08-07\"\ndftrain = df[df['ds'] &lt;= split_date] # Aug. 7th and earlier\ndftest = df[df['ds'] &gt; split_date] # Aug. 8th and later\n\n# verify split correctly\nprint(f\"dftrain shape: {dftrain.shape}\")\nprint(f\"dftest shape: {dftest.shape}\")\n\ndftrain shape: (15518, 2)\ndftest shape: (6939, 2)\n\n\n\n# initialize Prophet object and fit model\nmodel = Prophet()\nmodel.fit(dftrain)\n\n11:46:29 - cmdstanpy - INFO - Chain [1] start processing\n11:46:31 - cmdstanpy - INFO - Chain [1] done processing\n\n\n&lt;prophet.forecaster.Prophet at 0x1eec967e8d0&gt;\n\n\nThe make_future_dataframe() function is what allows us to set how far ahead our analysis will go. This creates a seperate data frame for our predictions.\n\n# make predictions\nfuture = model.make_future_dataframe(periods=20, freq='ME') # periods selects how far out the forecast goes\nforecast = model.predict(future)\n\n\n# plot predictions\nmodel.plot_components(forecast)\nplt.show()\n\n\n\n\n\n\n\n\n\nAnalysis\nOn the top graph of the result, there is a solid blue line that represents the actual values. The lighter blue shaded area represents the margin of error accounted for in predictions. Our model predicted the remaining dates in the test set and out till April 2025.\nThe dates closer to the split point have much less margin of error and it spreads out significantly the further ahead it goes. The further from the data we have, the less accurate the predictions will be. On the test set portion from August 8th, 2023, to December 31st, 2024, the model predicted it fairly well. Especially the points to the end of 2023 did really well.\nThe bottom graph represents the day of the week with the most sales on average. Clearly, mid-week (Wednesday) and the end of the week (Friday) expect to have more sales than the rest of the week. For the weekends, it’s harder to interpret because of the separation in the graph. But we can see that the massive dip in Saturday sales is recovered by Monday, indicating the possibility some of the dealerships are closed on the weekends."
  }
]